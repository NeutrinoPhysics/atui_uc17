{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle counting with CNNs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Always be aware of your imports and preserve namespaces!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.ndimage as nd\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"image.cmap\"] = \"gist_gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be similar to the last lecture, to count cars on the road way.  We'll concentrate on these two cameras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impath  = os.path.join(\"images\", \"dot_dl\")\n",
    "imlist0 = sorted(glob.glob(os.path.join(impath, \"cctv528*.jpg\")))\n",
    "imlist1 = sorted(glob.glob(os.path.join(impath, \"cctv679*.jpg\")))\n",
    "img0    = nd.imread(imlist0[0])\n",
    "img1    = nd.imread(imlist1[0])\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "[i.axis(\"off\") for i in ax]\n",
    "ims = [i.imshow(j) for i, j in zip(ax, (img0, img1))]\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have generate training/testing data consisting of $\\sim1000$ positive and $\\sim1000$ negative square postage stamps of sidelength $50$ (I am going to trim off a 1 pixel border for reasons that will become apparent later, making the side length 48).  Let's read those in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- get path to images\n",
    "stpath = os.path.join(\"images\", \"dl_training_lum\", \"*.npy\")\n",
    "stlist = sorted(glob.glob(stpath))\n",
    "nstamp = len(stlist)\n",
    "\n",
    "# -- set a shuffle index\n",
    "np.random.seed(314)\n",
    "sind = np.random.rand(nstamp).argsort()\n",
    "\n",
    "# -- read the postage stamps and shuffle\n",
    "stamps = np.array([np.load(i)[1:-1, 1:-1].flatten() for i in stlist]).astype(np.float32)[sind]\n",
    "npix   = stamps.shape[1]\n",
    "nside  = int(np.sqrt(npix))\n",
    "\n",
    "# -- set the labels\n",
    "labels = np.array([[1.0, 0.0] if \"pos\" in i  else [0.0, 1.0] for i in stlist]).astype(np.float32)[sind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nax = 5\n",
    "fig, ax = plt.subplots(nax, nax, figsize=(8, 8))\n",
    "[ax[ii // nax, ii % nax].axis(\"off\") for ii in range(nax * nax)]\n",
    "ims = [ax[ii // nax, ii % nax].imshow(stamps[labels[:, 0] == 1][ii].reshape(nside, nside)) for ii in range(nax * nax)]\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nax = 5\n",
    "fig, ax = plt.subplots(nax, nax, figsize=(8, 8))\n",
    "[ax[ii // nax, ii % nax].axis(\"off\") for ii in range(nax * nax)]\n",
    "ims = [ax[ii // nax, ii % nax].imshow(stamps[labels[:, 1] == 1][ii].reshape(nside, nside)) for ii in range(nax * nax)]\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now in **exactly** the same form as the MNIST data was, and so we can apply the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- prototype weight and bias variables\n",
    "def weight_variable(shape):\n",
    "    \"\"\" Initialize a variable with Gaussian noise.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\" Initialize a variable with a constant value.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# -- prototype convolutional and pooling functions\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "# -- define data prototypes and reshape input\n",
    "x       = tf.placeholder(tf.float32, shape=[None, npix])\n",
    "y_      = tf.placeholder(tf.float32, shape=[None, 2]) # only two possibilities car vs not car\n",
    "x_image = tf.reshape(x, [-1, 48, 48, 1]) # last channel is color channel\n",
    "\n",
    "\n",
    "# -- first layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "# -- second layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "# -- fully connected layer\n",
    "W_fc1        = weight_variable([12 * 12 * 64, 1024]) # this is why 48x48 instead of 50x50\n",
    "b_fc1        = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 12 * 12 * 64])\n",
    "h_fc1        = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "# -- dropout\n",
    "keep_prob  = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "# -- output layer\n",
    "W_fc2  = weight_variable([1024, 2])\n",
    "b_fc2  = bias_variable([2])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can train exactly as before.  The only difference is that we need a training and testing set, so let's do a 70/30 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = int(0.7 * nstamp)\n",
    "ntest  = nstamp - ntrain\n",
    "\n",
    "st_train = stamps[:ntrain]\n",
    "st_test  = stamps[ntrain:]\n",
    "lb_train = labels[:ntrain]\n",
    "lb_test  = labels[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- define the loss\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "# -- define the optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "# -- define prediction and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nepoch   = 10\n",
    "batch_sz = 50\n",
    "nstep    = ntrain // batch_sz\n",
    "loss     = np.zeros(nepoch * nstep)\n",
    "\n",
    "np.random.seed(1519)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # -- initialize **ALL** of those weights and biases\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # -- loop through epochs\n",
    "    for epoch in range(nepoch):\n",
    "\n",
    "        rind = np.random.rand(ntrain).argsort()\n",
    "        st_train = st_train[rind]\n",
    "        lb_train = lb_train[rind]\n",
    "        \n",
    "        for ii in range(nstep):\n",
    "            # get the next minibatch of images\n",
    "            lo = batch_sz * ii\n",
    "            hi = batch_sz *(ii + 1)\n",
    "            batch = [st_train[lo:hi], lb_train[lo:hi]]\n",
    "\n",
    "            # alert the user how the training is going after ever 100 epochs\n",
    "            if ii % 1 == 0:\n",
    "                acc_in = {x:batch[0], y_:batch[1], keep_prob:1.0} # don't use dropout for accuracy estimate\n",
    "                train_accuracy = accuracy.eval(feed_dict=acc_in)\n",
    "                print(\"epoch,step {0:2},{1:2} - training accuracy {2}\".format(epoch, ii, train_accuracy))\n",
    "\n",
    "            # take a gradient descent step\n",
    "            mod_in = {x:batch[0], y_:batch[1], keep_prob:0.5}\n",
    "            train_step.run(feed_dict=mod_in)\n",
    "            loss[epoch * nstep + ii] = sess.run(cross_entropy, mod_in)\n",
    "\n",
    "    # -- print the final accuracy on the test data\n",
    "    test_in = {x:st_test, y_:lb_test, keep_prob:1.0} # don't use dropout for testing\n",
    "    test_accuracy = accuracy.eval(feed_dict=test_in)\n",
    "    print(\"test accuracy {0}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- plot the loss\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.grid(1)\n",
    "plt.plot(np.arange(loss[loss > 0].size), np.log10(loss[loss > 0]))\n",
    "plt.xlabel(\"step\", fontsize=20)\n",
    "plt.ylabel(\"$\\log_{10}$ loss\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
