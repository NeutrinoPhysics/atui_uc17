{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit recognition with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Always be aware of your imports and preserve namespaces!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Loading the MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nimg, npix = mnist.train.images.shape\n",
    "\n",
    "print(\"shape of training data array    = {0}\".format(mnist.train.images.shape))\n",
    "print(\"dtype of training data array    = {0}\".format(mnist.train.images.dtype))\n",
    "print(\"min, max of training data array = {0}, {1}\".format(mnist.train.images.min(), mnist.train.images.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have $55,000$ training examples each with 784 features that are 32-bit floats with a minimum of 0 and a maximum of 1.  Of course, we know those features are pixels in the images (note that $\\sqrt{784} = 28$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nside = int(np.sqrt(npix))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "fig.subplots_adjust(0, 0, 1, 1)\n",
    "ax.axis(\"off\")\n",
    "im = ax.imshow(mnist.train.images[0].reshape(nside, nside), \"gist_gray\")\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What number is that???  $7$?  $3$?  The groundtruth is contained in the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"shape of the labels array             = {0}\".format(mnist.train.labels.shape))\n",
    "print(\"dtype of the labels array             = {0}\".format(mnist.train.labels.dtype))\n",
    "print(\"groundtruth vector for the 0'th image = {0}\".format(mnist.train.labels[0]))\n",
    "print(\"groundtruth answer for the 0'th image = {0}\".format(mnist.train.labels[0].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Models in Tensorflow, part 1: regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data, let's run some models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, computations are performed inside a \"session\".  That session can be run interactively (we'll see an example of a non-interactive session later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into CNN's let's start off with a simple regression model.  First we define the inputs and labels; these are TF \"placeholders\" and are to be input when we run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x  = tf.placeholder(tf.float32, shape=[None, npix])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, like any model, we need to define varaiables, i.e., the parameters to be fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, these variables **have no value**, they are not initialized.  We've just told TF that they exist not what they are yet (nb, this is similar to defining but not initializing a variable in C).  We can initialize them by running TF's function for initializing global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To what have those values been initialized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"W = {0}\".format(sess.run(W)))\n",
    "print(\"b = {0}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All zeros for the moment...  but that's OK, these parameters are to be solved for.\n",
    "\n",
    "Since everything is vectorized, the linear regression equation can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cross entropy loss function (note that we are now deviating from the more typical \"least squares\" regression solution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a model, and we've initialized some variables.  We can already calculate the model predictions and loss on some set of data.  Let's try the first 10 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(y, {x:mnist.train.images[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all zeros!  But what do these zeros mean in this conext, since surely this must be **some** number, right?  We can apply the softmax function to get the propabilities for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.nn.softmax(y, dim=-1), {x:mnist.train.images[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: with these values of $W$ and $b$ all values are equally likely for each image.\n",
    "\n",
    "And what's the loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex_img = mnist.train.images[:10]\n",
    "ex_lab = mnist.train.labels[:10]\n",
    "mod_in = {x:mnist.train.images[:10], y_:mnist.train.labels[:10]}\n",
    "\n",
    "print(\"loss for first 10 images is = {0}\".format(sess.run(cross_entropy, mod_in)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as with all machine learning, let's **learn** the parameters $W$ and $b$.  We'll use gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pass through this gradient descent optimizer takes exactly 1 step.  So we have to step many times to get near a minimum of the (cross entropy) loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nstep = 1000\n",
    "loss  = np.zeros(nstep)\n",
    "\n",
    "for ii in range(nstep):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1]})\n",
    "    loss[ii] = sess.run(cross_entropy, {x:batch[0], y_:batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "[i.grid(1) for i in ax]\n",
    "lin0, = ax[0].plot(np.arange(nstep) + 1, loss, color=\"darkred\")\n",
    "lin1, = ax[1].plot(np.log10(np.arange(nstep) + 1), loss, color=\"darkred\")\n",
    "ax[0].set_ylabel(\"cross entropy loss\")\n",
    "ax[0].set_xlabel(\"batch number\")\n",
    "ax[1].set_xlabel(\"$\\log_{10}$ batch number\")\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that W and b have been updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"W = {0}\".format(sess.run(W)))\n",
    "print(\"b = {0}\".format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our predictions for the first 10 are now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob  = sess.run(tf.nn.softmax(y, dim=-1), {x:mnist.train.images[:10]}).round(2)\n",
    "guess = prob.argmax(1)\n",
    "print(prob)\n",
    "for ii, jj in zip(guess, mnist.train.labels[:10].argmax(1)):\n",
    "    print(\"guess = {0}, truth = {1}\".format(ii, jj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that mnist.train.next_batch has shuffled things around a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "fig.subplots_adjust(0, 0, 1, 1)\n",
    "ax.axis(\"off\")\n",
    "im = ax.imshow(mnist.train.images[0].reshape(nside, nside), \"gist_gray\")\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model is doing a pretty good job on the (new) first 10 images of the training set.  How about the testing set which hasn't been used to modify $W$ and $b$?  We could write this as we did above and average over all examples, but let's use some TF code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "mod_in = {x:mnist.test.images, y_:mnist.test.labels}\n",
    "\n",
    "print(\"accuracy for the test set = {0}\".format(accuracy.eval(mod_in)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
